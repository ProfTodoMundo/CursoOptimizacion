% !TEX root = NotasCursoOptimizacion.tex

%===========================================
\section{Fundamentos}
%===========================================

%===========================================
\subsection{\'Algebra Lineal}
%===========================================

\begin{Def}[Valores y vectores propios]
Sea $A$ una matrix de $n\times n$, $v$ vector de dimensi\'on $n$ y $\lambda$ escalar. Se dice que $v$ es un vector propio de $A$ y $\lambda$ es vector propio de $A$ si se cumple
\begin{eqnarray*}
Av=\lambda v
\end{eqnarray*}
\end{Def}

\begin{Note}
De la igualdad anterior se tiene $Av-\lambda v=\left(A-I\lambda\right)v=0$, ecuaci\'on que siempre tiene como  soluci\'on $v=0$, para cualquier $\lambda$. Si $\left(A-I\lambda\right)$ es invertible, la \'unica soluci\'on es $v=0$. Para tener soluciones no triviales, se requiere que $\lambda$ sea tal que  $\left(A-I\lambda\right)$ no sea invertible, entonces
\begin{eqnarray*}
det\left(A-I\lambda\right)=0
\end{eqnarray*}
\end{Note}

\begin{Teo}
Sea $A$ matriz de $n\times n$, $\lambda$ es un valor propio de $A$ si y s\'olo si
\begin{eqnarray*}
p\left(\lambda\right)=det\left(A-I\lambda\right)=0
\end{eqnarray*}
\end{Teo}

\begin{Def}
La ecuaci\'ion $p\left(\lambda\right)=0$ se denomina polinomio caracter\'istico de $A$.
\end{Def}


\begin{Teo}
Sea $A$ una matriz de $n\times n$, y sean $\lambda_{1},\lambda_{2},\dots,\lambda_{m}$ valores caracter\'isticos de $A$ con vectores propios $v_{1},v_{2},\dots,v_{m}$. Entonces los vectores propios correspondientes a valores propios distintos son linealmente independientes.
\end{Teo}

\begin{Def}
Sea $\lambda$ valor propio de $A$, la multiplicidad geom\'etrica de $\lambda$ es la dimensi\'on del espacio propio correspondiente a $\lambda$.
\end{Def}

\begin{Def}Semejantes
Sean $A$ y $B$ matrices de $n\times n$ son semejantes si existe una matriz invertible $C$ de $n\times n$ tal que
\begin{eqnarray}
B=C^{-1}AC.
\end{eqnarray}
\end{Def}


\begin{Teo}
Sea $A\in\mathbb{R}^{n\times n}$ matriz sim\'etrica
\begin{itemize}
\item[i) ] Los valores propios de $A$ son reales.
\item[ii) ] Los vectores propios asociados a valores propios distintos son ortogonales.
\end{itemize}

\end{Teo}

\begin{Def}[Diagonalizacion]
Una matriz $A$ de $n\times n$ es diagonalizable si existe una matriz diagonal $D$ tal que $A$ es semejante a $D$.
\end{Def}


\begin{Prop}
Sea $A\in\mathbb{R}^{n\times n}$, $A$ es diagonalizable si y s\'olo si existe una matriz $P\in\mathbb{R}^{n\times n}$ no singular, tal que $D=P^{-1}\cdot A \cdot P$ donde $D\in\mathbb{R}^{n\times n}$ es una matriz diagonal.


\end{Prop}

\begin{Teo}Diagonalizacion
Una matriz $A$ de $n\times n$ es diagonalizable si y s\'olo si tiene $n$ valores propios linealmente independientes, en este caso la matriz diagonal $D$ tiene como elementos en la diagona los $n$ valores propios de $A$. Entonces $D=C^{-1}AC$.
\end{Teo}

\begin{Teo}
Sea $A$ matriz sim\'etrica real de $n\times n$ , entonces $A$ tiene $n$ vectores propios reales ortonormales.
\end{Teo}

\begin{Def}
Se dice que $A$ matriz de $n\times n$ es diagonalizable ortogonalmente si existe una matriz ortogonal $Q$ tal que $Q^{t}AQ=D$, donde $D=diag\left(\lambda_{1},\lambda{2},\dots,\lambda_{n}\right)$ son los valores propios de $A$.
\end{Def}

\begin{Teo}
Sea $A$ matriz real de $n\times n$, entonces $A$ es diagonalizable ortogonalmente si y s\'olo si $A$ es sim\'etrica.
\end{Teo}


\begin{Def}
Sea $A\in\mathbb{R}^{n\times n}$, se denomina \textbf{menor de orden $k$ de la matriz $A$} al determinante que se obtiene de eliminar $\left(n-k\right)$ filas y las mismas $\left(n-k\right)$ columnas.
\end{Def}


\begin{Def} Formas Cuadr\'aticas
\begin{itemize}
\item[i) ] Una ecuaci\'on cuadr\'atica en dos variables sin t\'erminos lineales es una ecuaci\'on de la forma $ax^{2}+bxy+cy^{2}=d$, donde $|a|+|b|+|c|\neq0$.

\item[ii) ] Una forma cuadr\'atica en dos variables es una expresi\'on de la forma $F\left(x,y\right)=ax^{2}+bxy+cy^{2}$, donde $|a|+|b|+|c|\neq0$.
\end{itemize}
\end{Def}


\begin{Def} Sobre formas cuadr\'aticas
Sea $A$ matriz sim\'etrica, entonces se define la forma cuadr\'atica $F\left(x,y\right)=Av\cdot v$.
\end{Def}

\begin{Ejem}
Si $F\left(x,y\right)=ax^{2}+bxy+cy^{2}$ es forma cuadr\'atica, sea $A=\left(\begin{array}{cc}
a & b/2\\
b/2 & a\\
\end{array}
\right)$
entonces $Av\cdot v=d$.
\end{Ejem}


\begin{Def}
Una forma cuadr\'atica $\varphi\left(x\right)$ es una aplicaci\'on $\varphi:\mathbb{R}^{n}\rightarrow\mathbb{R}$ tal que
\begin{eqnarray*}
\varphi\left(x\right)=\varphi\left(x_{1},x_{2},\dots,x_{n}\right)=\sum_{i}a_{ii}x_{i}^{2}+2\sum_{i\leq j}a_{ij}x_{i}x_{j}.
\end{eqnarray*}
La matriz asociada a la forma cuadr\'atica $\varphi\left(x\right)$ es una matriz $n\times n$ y sim\'etricade la forma: $\varphi\left(x\right)=x^{t}Ax$.
\end{Def}

\begin{Note}
La matriz $A$ asociada a una forma cuadr\'atica es $n\times n$ y sim\'etrica, por tanto diagonalizable ortogonalmente, es decir, existe una matriz $Q\in \mathbb{R}^{n\times n}$ ortogonal tal que $D=Q^{t}AQ$, donde $D\in\mathbb{R}^{n\times n}$ es una matriz diagonal, donde los elementos de la diagonal son los valores propios $\lambda_{i}$ de $A$, y la columnas de $Q$ son los vectores propios de $A$.

De donde
\begin{eqnarray*}
&&D=Q^{t}AQ,\textrm{ por tanto }A=QDQ^{t}, \textrm{ entonces la forma cuadr\'atica queda de la forma }\\
&&\varphi\left(x\right)=x^{t}Ax=x^{t}QDQ^{t}x=\left(x^{t}Q\right)D\left(Q^{t}x\right)=\left(Q^{t}x\right)^{t}D\left(Q^{t}x\right),\textrm{ haciendo  }\tilde{x}=Q^{t}x\\
&&\varphi\left(x\right)=\tilde{\varphi}\left(\tilde{x}\right)=\tilde{x}^{t}D\tilde{x}=\sum_{i=1}^{n}\lambda_{i}\tilde{x}^{2}.
\end{eqnarray*}
Se dice que $\tilde{\varphi}\left(\tilde{x}\right)$ es la forma can\'onica o forma diagonal de la forma cuadr\'atica $\varphi\left(x\right)$.
\end{Note}

\begin{Teo}
Sea $A\in\mathbb{R}^{n\times n}$ sim\'etrica y real; sean $A_{i},1\leq1\leq n$ los menores principales de orden $i$ de la matriz $n$, entonces
\begin{itemize}
\item[i) ] $A$ es definida positiva si y s\'olo s\'i todos los menores principales son positivos.

\item[ii) ] $A$ es definida negativa s\'i y s\'olo s\'i los menores principales de orden impar son negativos y los de orden par son positivos.

\item[iii) ] Si $A$ es semidefinida positiva o negativa, entonces $det(A)=0$.
\item[iv) ] Si $A_{1}>0$, $A_{2}>0,\dots$,$A_{n}>0$ y $det(A)=0$, entonces $A$ es semidefinida positiva.
\item[v) ] Si $A_{1}<0$, $A_{2}>0,A_{3}<0,\dots$, y $det(A)=0$, entonces $A$ es semidefinida negativa.
\item[vi) ] $A$ es semidefinida positiva s\'i y s\'olo s\'i todos los menores de la matriz $A$ son mayores o iguales a cero.
\item[vii) ] $A$ es semidefinida negaiva s\'i y s\'olo s\'i  los menores de la matriz $A$ de orden impar son menores o iguales a cero y los de orden par son mayores o iguales a cero.
\item[viii) ] Si no se cumplen estas condiciones, la matriz $A$ es indefinida.


\end{itemize}

\end{Teo}


\begin{Def} [Segmento]
Sean $x,y\in\mathbb{R}^{n}$, se llama \textbf{segmento} de extremos $x$ e $y$ al conjunto de puntos $z\in\mathbb{R}^{n}$ tales que $z=ax+\left(1-\alpha\right)y$ con $0\leq\alpha\leq1$.
\end{Def}

\begin{Def} [Convexo]
Un conjunto $S\subseteq\mathbb{R}^{n}$ es \textbf{convexo} si $\forall x,y\in S$ y $\forall\alpha\in\left[0,1\right]$ se cumple que $\alpha x+\left(1-\alpha\right)y\in S$.
\end{Def}


\begin{Def} [Funciones convexas] Sea $f:D\rightarrow\mathbb{R}$, con $D\subset \mathbb{R}^{n}$ conjunto convexo no vac\'io.
\begin{itemize}

\item[a) ] $f$ es convexa en $D$ s\'i y s\'olo s\'i $f\left(\alpha x +\left(1-\alpha\right)y\right)\leq\alpha f\left(x\right)+\left(1-\alpha\right)f\left(x\right)$ $\forall x,y\in D$, $\forall \alpha\in\left[0,1\right]$.

\item[b)] $f$ es estrictamente convexa en $D$ s\'i y s\'olo s\'i $f\left(\alpha x +\left(1-\alpha\right)y\right)<\alpha f\left(x\right)+\left(1-\alpha\right)f\left(x\right)$ $\forall x,y\in D$, $\forall \alpha\in\left(0,1\right)$.

\item[c) ] $f$ es c\'oncava s\'i y s\'olo s\'i $f\left(\alpha x +\left(1-\alpha\right)y\right)\geq\alpha f\left(x\right)+\left(1-\alpha\right)f\left(x\right)$, $\forall x,y\in D$, $\forall \alpha\in\left[0,1\right]$.

\item[d) ] $f$ es c\'oncava s\'i y s\'olo s\'i $f\left(\alpha x +\left(1-\alpha\right)y\right)>\alpha f\left(x\right)+\left(1-\alpha\right)f\left(x\right)$, $\forall x,y\in D$, $\forall \alpha\in\left(0,1\right)$.



\end{itemize}

\end{Def}

\begin{Teo} Sobre funciones convexas
Sea $f:D\rightarrow\mathbb{R}$, con $D\subset \mathbb{R}^{n}$ conjunto convexo no vac\'io.
\begin{itemize}
\item[i) ] $f$ es convexa en $D$ s\'i y s\'olo s\'i la matriz $Hf\left(x\right)$ es semidefinida o definida positiva $\forall x\in D$.

\item[ii) ] $f$ es c\'oncava en $D$ s\'i y s\'olo s\'i la matriz $Hf\left(x\right)$ es semidefinida o definida negativa $\forall x\in D$.

\item[iii) ] Si la matriz $Hf\left(x\right)$ es definida positiva $\forall x\in D$, entonces $f$ es estr\'ictamente convexa en $D$

\item[iii) ] Si la matriz $Hf\left(x\right)$ es definidanegativa $\forall x\in D$, entonces $f$ es estr\'ictamente c\'oncava en $D$
\end{itemize}

La matrix $Hf\left(x\right)$ se denomina matriz Hessiana de la funci\'on $f$:
\begin{eqnarray*}
Hf\left(x\right)=Hf\left(x_1,x_{2},\dots,x_{n}\right)=\left(\begin{array}{ccc}
f^{(2)}_{x_1 x_1} & \cdots &f^{(2)}_{x_1 x_n}\\ 
\vdots &\ddots & \vdots\\
f^{(2)}_{x_n x_1} & \cdots &f^{(2)}_{x_n x_n}\\ 
\end{array}\right)
\end{eqnarray*}

\end{Teo}



\begin{Propty} [Funciones convexas]
\begin{itemize}
\item[i) ] Sea $f : D \to \mathbb{R}$, $D \subset \mathbb{R}^n$.  
Si $f$ es c\'oncava (convexa) en $D$, entonces es continua en el interior de $D$.

\item[ii) ] Sea $f : D \to \mathbb{R}$, $D \subset \mathbb{R}^n$ conjunto convexo no vac\'io.
\begin{itemize}
\item[a) ] Si $f$ es estrictamente convexa (estrictamente c\'oncava) en $D$, entonces $f$ es convexa (c\'oncava) en $D$.

\item[b) ] $f$ es convexa (estrictamente convexa) en $D$ si y s\'olo si $-f$ es c\'oncava (estrictamente c\'oncava) en $D$.

\item[c) ] Si $f$ es convexa (c\'oncava) en $D$ y $\alpha \ge 0$, entonces $\alpha f$ es convexa (c\'oncava) en $D$.

\item[d) ] Si $f(x) > 0$ y c\'oncava en $D$, entonces
$g(x) = \frac{1}{f(x)}$ es convexa en $D$.
\end{itemize}

\item[iii) ] Sean $f_i : D \to \mathbb{R}$ $(1 \leq i \leq k)$, $D \subset \mathbb{R}^n$ conjunto convexo no vac\'io, funciones convexas (c\'oncavas) en $D$.
\begin{itemize}
\item[a) ] La suma de dichas funciones es una funci\'on convexa (c\'oncava) en $D$.

\item[b) ] Si $\alpha_1, \ldots, \alpha_k$ son escalares no negativos, entonces $\sum_{i=1}^k \alpha_i f_i$ es una funci\'on convexa (c\'oncava) en $D$.
\end{itemize}

\item[iv) ] Sea $f : D \to \mathbb{R}$, $D \subset \mathbb{R}^n$ conjunto convexo no vac\'io.
\begin{itemize}
\item[a) ] Si $f$ es convexa en $D$, entonces $S_\alpha = \{x \in D \mid f(x) \le \alpha\}$
es un conjunto convexo para todo $\alpha \in \mathbb{R}$.

\item[b) ] Si $f$ es c\'oncava en $D$, entonces $T_\alpha = \{x \in D \mid f(x) \ge \alpha\}$
es un conjunto convexo para todo $\alpha \in \mathbb{R}$.
\end{itemize}

\item[v) ] Sea $f : D \to \mathbb{R}$, $D \subset \mathbb{R}^n$ conjunto convexo no vac\'io y sea $g : E \to \mathbb{R}$ tal que $\mathrm{Im}\, f \subset E \subset \mathbb{R}$.
\begin{itemize}
\item[a) ] Si $f$ es convexa y $g$ es creciente y convexa, entonces $g \circ f$ es convexa en $D$.

\item[b) ] Si $f$ es convexa y $g$ es decreciente y c\'oncava, entonces $g \circ f$ es c\'oncava en $D$.

\item[c) ] Si $f$ es c\'oncava y $g$ es decreciente y convexa, entonces $g \circ f$ es convexa en $D$.

\item[) ] Si $f$ es c\'oncava y $g$ es creciente y c\'oncava, entonces $g \circ f$ es c\'oncava en $D$.
\end{itemize}
\end{itemize}

\end{Propty}

\subsubsection{Ejercicios}

\begin{enumerate}
\item
\end{enumerate}


%===========================================
\subsection{Optimizaci\'on}
%===========================================

\begin{Def}[Extremos locales y globales]
\begin{itemize}
\item[i) ] Una funci\'on $f(x)=f(x_1,\ldots,x_n)$ alcanza un \textit{m\'aximo relativo o local} en un punto $x_0=(x_1^0,\ldots,x_n^0)$ de su dominio si se verifica que $f(x)\le f(x_0)$ para todo punto $x$ perteneciente a una vecindad de $x_0$.

\item[ii) ] Una funci\'on $f(x)=f(x_1,\ldots,x_n)$ alcanza un \textit{m\'inimo relativo o local} en un punto  $x_0=(x_1^0,\ldots,x_n^0)$ de su dominio si se verifica que $f(x)\ge f(x_0)$ para todo punto $x$ perteneciente a un entorno de $x_0$.
\end{itemize}
\end{Def}

\begin{Obs}
Si las desigualdades de las definiciones anteriores se cumplen para todos los puntos $x$ pertenecientes al dominio de la funci\'on $f$, entonces $f$ alcanza un \textit{m\'aximo absoluto o global} (o \textit{m\'inimo absoluto o global}) en el punto $x_0$.
\end{Obs}


Un problema de optimizaci\'on sin restricciones se puede formular de la siguiente manera:
\begin{eqnarray*}
\textrm{Optimizar } f(x) \textrm{ donde } f:D\to\mathbb{R}, \textrm{ con }D\subseteq\mathbb{R}^n.
\end{eqnarray*}


\subsection*{Condici\'on necesaria para la existencia de \'optimos locales}

\begin{Teo} Sea $f:D\to\mathbb{R}$ con $D\subseteq\mathbb{R}^n$ abierto, $f\in C^1(D)$. Si en $x_0\in D$ la funci\'on $f$ presenta un \'optimo local, entonces $\nabla f(x_0)=0$, es decir, $f'_{x_1}(x_0)=0,\ldots,f'_{x_n}(x_0)=0$.
\end{Teo}

\begin{Def} Sea $f:D\to\mathbb{R}$ con $D\subseteq\mathbb{R}^n$ abierto, $f\in C^1(D)$ y $x_0\in D$. Se dice que $x_0$ es un \textit{punto cr\'itico} de $f$ si $\nabla f(x_0)=0$.
\end{Def}


\begin{Teo} Sea $f:D\to\mathbb{R}$ con $D\subseteq\mathbb{R}^n$ abierto, $f\in C^2(D)$ y sea $x_0\in D$ un punto cr\'itico de $f$. Se cumple lo siguiente:

\begin{itemize}
\item[(i)] Si la forma cuadr\'atica dada por $Hf(x_0)$ es definida positiva, entonces $f$ presenta en $x_0$ un m\'inimo local.
\item[(ii)] Si la forma cuadr\'atica dada por $Hf(x_0)$ es definida negativa, entonces $f$ presenta en $x_0$ un m\'aximo local.
\item[(iii)] Si la forma cuadr\'atica dada por $Hf(x_0)$ es indefinida, entonces $f$ presenta en $x_0$ un punto silla.
\end{itemize}
\end{Teo}


\begin{Obs} Si $x_0$ es un punto cr\'itico de $f$ pero $f$ no tiene un \'optimo local en $x_0$, se dice que $x_0$ es un \textit{punto silla} o \textit{punto de ensilladura} de $f$, es decir, $x_0$ es un punto silla de $f$ si y s\'olo si existen puntos $x$ e $y$ en una vecindad de $x_0$ tales que  $f(x)<f(x_0)<f(y)$.
\end{Obs}

\begin{Teo} Sea $f:D\to\mathbb{R}$ con $D\subseteq\mathbb{R}^n$ abierto y convexo, $f\in C^1(D)$ y sea $x_0\in D$ un punto cr\'itico de $f$. Entonces se cumple lo siguiente:

\begin{itemize}
\item[(i)] Si $f$ es convexa en $D$, entonces $f$ presenta en $x_0$ un m\'inimo global.
\item[(ii)] Si $f$ es estrictamente convexa en $D$, entonces $f$ presenta en $x_0$ un m\'inimo global \'unico.
\item[(iii)] Si $f$ es c\'oncava en $D$, entonces $f$ presenta en $x_0$ un m\'aximo global.
\item[(iv)] Si $f$ es estrictamente c\'oncava en $D$, entonces $f$ presenta en $x_0$ un m\'aximo global \'unico.
\end{itemize}
\end{Teo}

\begin{Obs} Si $f$ es c\'oncava (convexa) en $D$, con $D$ abierto y convexo y $f\in C^1(D)$, la condici\'on $\nabla f(x_0)=0$ para $x_0\in D$ es necesaria y suficiente para que la funci\'on $f$ alcance en $x_0$ un m\'aximo(m\'inimo) global.
\end{Obs}

\begin{Prop} Sea $f:D\to\mathbb{R}$ con $D\subseteq\mathbb{R}^n$ abierto, y sea $g:A\to\mathbb{R}$  tal que $Im\left(f\right)\subset A\subset\mathbb{R}$. $f\in C^1(D)$ y sea $x_0\in D$ un punto cr\'itico de $f$. Entonces se cumple que:

\begin{itemize}
\item[(i)] Si $g$ es creciente, entonces los puntos donde $f$ alcanza m\'aximos (m\'inimos) en $D$ coinciden con los puntos donde $g\circ f$ alcanza m\'aximos (m\'inimos) en $D$.
\item[(ii)] Si $g$ es decreciente, entonces los puntos donde $f$ alcanza m\'aximos (m\'inimos) en $D$ coinciden con los puntos donde $g\circ f$ alcanza m\'inimos (m\'aximos) en $D$.
\end{itemize}
\end{Prop}

\subsection{Ejercicios}


\begin{Ejer}
Dada la siguiente matriz:
\[
A=
\begin{pmatrix}
2 & 0 & 6\\
0 & 0 & 1\\
1 & 0 & 1
\end{pmatrix}
\]

Hallar los autovalores y autovectores de la matriz $A$.
\end{Ejer}

\begin{Ejer}
Dada la siguiente matriz:
\[
A=
\begin{pmatrix}
1 & 0 & 1\\
0 & 1 & 3\\
0 & 0 & 2
\end{pmatrix}
\]

Hallar los autovalores y autovectores de la matriz $A$.
\end{Ejer}


\begin{Ejer}

Hallar los autovalores y autovectores de la siguiente matriz simétrica:
\[
A=
\begin{pmatrix}
7 & -2 & 1\\
-2 & 10 & -2\\
1 & -2 & 7
\end{pmatrix}
\]
\end{Ejer}

\begin{Ejer}
Dada la siguiente matriz indicar si es diagonalizable. 
En caso de que lo sea, calcular las matrices $D$ y $P$ que verifican:
\[
D = P^{-1} A P
\]

\[
A=
\begin{pmatrix}
1 & 0 & 1\\
0 & 1 & 3\\
0 & 0 & 2
\end{pmatrix}
\]

Dada la siguiente matriz indicar si es diagonalizable. 
En caso de que lo sea, calcular las matrices $D$ y $P$ que verifican:
\[
D = P^{-1} A P
\]

\[
A=
\begin{pmatrix}
-2 & -1 & -1\\
4 & -8 & -6\\
-4 & 11 & 9
\end{pmatrix}
\]
\end{Ejer}

\begin{Ejer}
Dada la siguiente matriz simétrica $A$:
\[
A=
\begin{pmatrix}
7 & -2 & 1\\
-2 & 10 & -2\\
1 & -2 & 7
\end{pmatrix}
\]

Determinar la matriz diagonal $(D)$ y la matriz ortogonal $(Q)$ que verifican:
\[
D = Q^t A Q
\]
\end{Ejer}


\begin{Ejer}

Dada la siguiente matriz simétrica $A$:
\[
A=
\begin{pmatrix}
7 & -2 & 1\\
-2 & 10 & -2\\
1 & -2 & 7
\end{pmatrix}
\]

Determinar la matriz diagonal $(D)$ y la matriz ortogonal $(Q)$ que verifican:
\[
D = Q^t A Q
\]
\end{Ejer}

\begin{Ejer}
a) Buscar la expresión polinómica de la forma cuadrática asociada a la siguiente matriz:

\[
A=
\begin{pmatrix}
2 & -1 & \frac{3}{2}\\
-1 & 0 & -2\\
\frac{3}{2} & -2 & -1
\end{pmatrix}
\]



b) Buscar la matriz asociada a la siguiente forma cuadrática

\[
\varphi(x_1,x_2,x_3)
= 3x_1^2 - x_2^2 + 4x_3^2 + 5x_1x_2 - 6x_1x_3 - x_2x_3
\]
\end{Ejer}



\begin{Ejer}
Determinar el signo de las siguientes formas cuadráticas a partir del signo de sus autovalores:

a) $\varphi(x_1,x_2)=3x_1^2+x_2^2$ es definida positiva

\[
A=
\begin{pmatrix}
3 & 0\\
0 & 1
\end{pmatrix}
\]

Autovalores:
\[
\lambda_1=3>0,
\qquad
\lambda_2=1>0
\]

b) $\varphi(x_1,x_2)=(x_1-x_2)^2=x_1^2-2x_1x_2+x_2^2$ es semidefinida positiva

\[
A=
\begin{pmatrix}
1 & -1\\
-1 & 1
\end{pmatrix}
\]

Autovalores:
\[
\lambda_1=2>0,
\qquad
\lambda_2=0
\]

c) $\varphi(x_1,x_2,x_3)=-7x_1^2-2x_2^2-4x_3^2$ es definida negativa

\[
A=
\begin{pmatrix}
-7 & 0 & 0\\
0 & -2 & 0\\
0 & 0 & -4
\end{pmatrix}
\]

Autovalores:
\[
\lambda_1=-7<0,\quad
\lambda_2=-2<0,\quad
\lambda_3=-4<0
\]

d) $\varphi(x_1,x_2,x_3)=-3x_1^2-(x_2-2x_3)^2
= -3x_1^2-x_2^2+4x_2x_3-4x_3^2$ es semidefinida negativa

\[
A=
\begin{pmatrix}
-3 & 0 & 0\\
0 & -1 & 2\\
0 & 2 & -4
\end{pmatrix}
\]

Autovalores:
\[
\lambda_1=-3<0,\quad
\lambda_2=-5<0,\quad
\lambda_3=0
\]

e) $\varphi(x_1,x_2)=5x_1^2-7x_2^2$ es indefinida

\[
A=
\begin{pmatrix}
5 & 0\\
0 & -7
\end{pmatrix}
\]

Autovalores:
\[
\lambda_1=5>0,\qquad
\lambda_2=-7<0
\]
\end{Ejer}


\begin{Ejer}
Determinar el signo de las siguientes formas cuadráticas a partir del signo de sus menores principales:

a) $\varphi(x_1,x_2)=3x_1^2+x_2^2$ es definida positiva

\[
A=
\begin{pmatrix}
3 & 0\\
0 & 1
\end{pmatrix}
\]

Los menores principales:
\[
A_1=3>0,\qquad
A_2=\det(A)=3>0
\]

b) $\varphi(x_1,x_2)=(x_1-x_2)^2$ es semidefinida positiva

\[
A=
\begin{pmatrix}
1 & -1\\
-1 & 1
\end{pmatrix}
\]

Los menores principales:
\[
A_1=1>0,\qquad
A_2=\det(A)=0
\]

c) $\varphi(x_1,x_2,x_3)=-7x_1^2-2x_2^2-4x_3^2$ es definida negativa

\[
A=
\begin{pmatrix}
-7 & 0 & 0\\
0 & -2 & 0\\
0 & 0 & -4
\end{pmatrix}
\]

Los menores principales:
\[
A_1=-7<0,\quad
A_2=14>0,\quad
A_3=\det(A)=-56<0
\]

d) $\varphi(x_1,x_2,x_3)=-3x_1^2-(x_2-2x_3)^2$ es semidefinida negativa

\[
A=
\begin{pmatrix}
-3 & 0 & 0\\
0 & -1 & 2\\
0 & 2 & -4
\end{pmatrix}
\]

Los menores principales:
\[
A_1=-3<0,\quad
A_2=3>0,\quad
A_3=\det(A)=0
\]

e) $\varphi(x_1,x_2)=5x_1^2-7x_2^2$ es indefinida

\[
A=
\begin{pmatrix}
5 & 0\\
0 & -7
\end{pmatrix}
\]

Los menores principales:
\[
A_1=5>0,\qquad
A_2=\det(A)=-35<0
\]
\end{Ejer}

\begin{Ejer}

Analizar el signo de la siguiente forma cuadrática

\[
\varphi(x_1,x_2,x_3)
= -x_1^2 + 6x_1x_2 - 9x_2^2 - 2x_3^2
\]

La matriz asociada es

\[
A=
\begin{pmatrix}
-1 & 3 & 0\\
3 & -9 & 0\\
0 & 0 & -2
\end{pmatrix}
\]



Los menores principales son

\[
\begin{cases}
A_1 = -1 < 0 \\
A_2 = \det
\begin{pmatrix}
-1 & 3\\
3 & -9
\end{pmatrix}
= 0 \\
A_3 = \det(A) = 0
\end{cases}
\]


\bigskip

Sea

\[
f(x,y)=\frac{1}{x}+\frac{1}{y},
\qquad (x>0 \ \wedge \ y>0)
\]

Calculamos las derivadas parciales

\[
\begin{cases}
f_x = -\frac{1}{x^2} \\
f_y = -\frac{1}{y^2}
\end{cases}
\]

La matriz Hessiana es:

\[
Hf(x,y)=
\begin{pmatrix}
\dfrac{2}{x^3} & 0 \\
0 & \dfrac{2}{y^3}
\end{pmatrix}
\]

¿C\'omo se determina el signo de la matriz Hessiana?
\end{Ejer}

\begin{Ejer}

\[
f(x_1,x_2,x_3)
= (x_1-1)^2 + 2x_2^3 + 5x_3^2
\]

Calculamos las derivadas parciales

\[
\begin{cases}
f_{x_1} = 2(x_1-1) \\
f_{x_2} = 6x_2^2 \\
f_{x_3} = 10x_3
\end{cases}
\]

La matriz Hessiana es:

\[
Hf(x_1,x_2,x_3)=
\begin{pmatrix}
2 & 0 & 0 \\
0 & 12x_2 & 0 \\
0 & 0 & 10
\end{pmatrix}
\]

Observemos que si se considera $D=\mathbb{R}^3$ la función no es c\'oncava ni convexa, 
pero si consideramos como dominio

\[
D=\{(x_1,x_2,x_3)\in\mathbb{R}^3 \mid x_2>0\}
\]

(conjunto abierto y convexo) entonces $f$ es estrictamente convexa en $D$.
\end{Ejer}

\begin{Ejer}

\[
f(x,y)=x^2+y^2+xy-2x-10y+5
\]

Hallamos las derivadas parciales primeras, las igualamos a cero y resolvemos el sistema:

\[
\begin{cases}
f_x = 2x+y-2=0\\
f_y = 2y+x-10=0
\end{cases}
\quad \Longrightarrow \quad (-2,6) \text{ es el punto crítico}
\]

Ahora hallamos las derivadas parciales segundas y construimos la matriz Hessiana:

\[
Hf(x,y)=
\begin{pmatrix}
2 & 1\\
1 & 2
\end{pmatrix}
\]

Los menores principales son:

\[
\begin{cases}
|H_1|=2>0\\
|H_2|=\det(Hf)=3>0
\end{cases}
\]

Como la matriz Hessiana es definida positiva concluimos que la función presenta en 
$(-2,6)$ un mínimo relativo.

Observemos que otra forma de expresar la solución hubiese sido: en 
$(-2,6,f(-2,6))=(-2,6,-23)$ hay un mínimo relativo.
\end{Ejer}


\begin{Ejer}

\[
f(x,y)=e^{x^2+y^2}
\]

Hallamos las derivadas parciales primeras, las igualamos a cero y resolvemos el sistema:

\[
\begin{cases}
f_x = 2xe^{x^2+y^2}=0\\
f_y = 2ye^{x^2+y^2}=0
\end{cases}
\quad \Longrightarrow \quad (0,0) \text{ es el punto crítico}
\]

Ahora hallamos las derivadas parciales segundas y construimos la matriz Hessiana:

\[
Hf(x,y)=
\begin{pmatrix}
(4x^2+2)e^{x^2+y^2} & 4xye^{x^2+y^2}\\
4xye^{x^2+y^2} & (4y^2+2)e^{x^2+y^2}
\end{pmatrix}
\]

Reemplazamos por el punto crítico:

\[
Hf(0,0)=
\begin{pmatrix}
2 & 0\\
0 & 2
\end{pmatrix}
\]

Los menores principales son:

\[
\begin{cases}
|H_1|=2>0\\
|H_2|=\det(Hf)=4>0
\end{cases}
\]

Como la matriz Hessiana es definida positiva concluimos que la función presenta en 
$(0,0)$ un mínimo relativo.
\end{Ejer}

\begin{Ejer}

\[
f(x_1,x_2,x_3)= -x_1^3 + 3x_1x_3 + 2x_2 - x_2^2 - 3x_3^2
\]

Hallamos las derivadas parciales primeras, las igualamos a cero y resolvemos el sistema:

\[
\begin{cases}
f_{x_1} = -3x_1^2 + 3x_3 = 0\\
f_{x_2} = 2 - 2x_2 = 0\\
f_{x_3} = 3x_1 - 6x_3 = 0
\end{cases}
\]

Los puntos críticos son

\[
P_1=\left(\frac{1}{2},1,\frac{1}{4}\right)
\quad \text{y} \quad
P_2=(0,1,0).
\]
\end{Ejer}

\begin{Ejer}

Consideremos la función 
\[
f(x,y)=e^{x^2+y^2}
\]
y sea 
\[
g(x)=\ln x
\]
(función estrictamente creciente).

Busco los extremos de la función:

\[
h(x,y)=g\circ f(x,y)=x^2+y^2
\]

\[
\begin{cases}
h_x = 2x=0\\
h_y = 2y=0
\end{cases}
\quad \Longrightarrow \quad (0,0) \text{ es el punto crítico}
\]

La matriz Hessiana es:

\[
Hh(x,y)=
\begin{pmatrix}
2 & 0\\
0 & 2
\end{pmatrix}
\]

Como la matriz Hessiana es definida positiva concluimos que la función $h$ presenta en 
$(0,0)$ un mínimo relativo.

Luego en ese punto la función $f$ también alcanza un mínimo relativo.
\end{Ejer}


\begin{Ejer}

\[
f(x,y)=x^2+y^2+xy-2x-10y+5
\]

Derivadas primeras:
\[
\begin{cases}
f'_x=2x+y-2=0\\
f'_y=2y+x-10=0
\end{cases}
\Rightarrow (-2,6)
\]

Matriz Hessiana:
\[
Hf(x,y)=
\begin{pmatrix}
2 & 1\\
1 & 2
\end{pmatrix}
\]

Menores principales:
\[
|H_1|=2>0,\qquad |H_2|=\det Hf=3>0
\]

Luego $(-2,6)$ es un m\'inimo relativo.
\end{Ejer}

\begin{Ejer}
\[
f(x,y)=2-3x^2+3x^2y+y^3-3y^2
\]

Puntos cr\'iticos:
\[
P_1=(0,0),\; P_2=(0,2),\; P_3=(1,1),\; P_4=(-1,1)
\]

Matriz Hessiana:
\[
Hf(x,y)=
\begin{pmatrix}
-6+6y & 6x\\
6x & 6y-6
\end{pmatrix}
\]

Clasificaci\'on:
\[
P_1:\text{m\'aximo},\quad
P_2:\text{m\'inimo},\quad
P_3,P_4:\text{puntos silla}
\]
\end{Ejer}


\begin{Ejer}
\[
f(x,y)=e^{x^2+y^2}
\]

Punto cr\'itico:
\[
(0,0)
\]

Hessiana:
\[
Hf(0,0)=
\begin{pmatrix}
2 & 0\\
0 & 2
\end{pmatrix}
\Rightarrow \text{m\'inimo relativo}
\]
\end{Ejer}

\begin{Ejer}
\[
f(x_1,x_2,x_3)=-x_1^3+3x_1x_3+2x_2-x_2^2-3x_3^2
\]

Puntos cr\'iticos:
\[
P_1=\left(\tfrac12,1,\tfrac14\right),\quad P_2=(0,1,0)
\]

Clasificaci\'on:
\[
P_1:\text{m\'aximo relativo},\quad P_2:\text{punto silla}
\]
\end{Ejer}


\begin{Ejer}

\[
\begin{cases}
f(x,y)=xy\\
x+y=12
\end{cases}
\]

Funci\'on de Lagrange:
\[
\mathcal{L}(x,y,\lambda)=xy+\lambda(12-x-y)
\]

Condiciones de primer orden:
\[
\begin{cases}
\mathcal{L}'_x=y-\lambda=0\\
\mathcal{L}'_y=x-\lambda=0\\
\mathcal{L}'_\lambda=12-x-y=0
\end{cases}
\]

De las dos primeras ecuaciones:
\[
\lambda=y,\quad \lambda=x \Rightarrow y=x
\]

Reemplazando:
\[
12-x-x=0 \Rightarrow x=6,\; y=6,\; \lambda=6
\]

Punto cr\'itico $P=(6,6)$.

Matriz Hessiana orlada:
\[
\bar H=
\begin{pmatrix}
0 & 1 & 1\\
1 & 0 & 1\\
1 & 1 & 0
\end{pmatrix}
\]

Como $n=2$, $m=1$ entonces $n-m=1$.

Para m\'aximo relativo:
\[
|\bar H|>0
\]

Como $\det(\bar H)=2>0$, el problema presenta en $P=(6,6)$ un m\'aximo relativo.
\end{Ejer}


%-------------------------------------------------
\begin{Ejer}

\[
\begin{cases}
f(x,y)=3x+2y\\
2x^2+3y^2=210
\end{cases}
\]

Funci\'on de Lagrange:
\[
\mathcal{L}(x,y,\lambda)=3x+2y+\lambda(210-2x^2-3y^2)
\]

Condiciones de primer orden:
\[
\begin{cases}
3-4\lambda x=0\\
2-6\lambda y=0\\
210-2x^2-3y^2=0
\end{cases}
\]

De las dos primeras ecuaciones:
\[
\lambda=\frac{3}{4x}=\frac{1}{3y} \Rightarrow y=\frac{4}{9}x
\]

Reemplazando en la restricci\'on:
\[
2x^2+3\left(\frac{4}{9}x\right)^2=210
\Rightarrow x^2=81
\]

Puntos cr\'iticos:
\[
P_1=(9,4),\;\lambda=\frac{1}{12}
\qquad
P_2=(-9,-4),\;\lambda=-\frac{1}{12}
\]

Matriz Hessiana orlada:
\[
\bar H=
\begin{pmatrix}
0 & 4x & 6y\\
4x & -4\lambda & 0\\
6y & 0 & -6\lambda
\end{pmatrix}
\]

Evaluando:
\[
\det\bar H(P_1)=840>0 \Rightarrow \text{m\'aximo relativo}
\]
\[
\det\bar H(P_2)=-840<0 \Rightarrow \text{m\'inimo relativo}
\]
\end{Ejer}

%-------------------------------------------------
\begin{Ejer}

\[
\begin{cases}
f(x,y,z)=x^2+y^2+z^2\\
x+y=2
\end{cases}
\]

Funci\'on de Lagrange:
\[
\mathcal{L}(x,y,z,\lambda)=x^2+y^2+z^2+\lambda(2-x-y)
\]
\end{Ejer}
%-------------------------------------------------
\begin{Ejer}

\[
\begin{cases}
f(x,y,z)=x+2y+2z\\
x^2+y^2=25\\
x+y+z=0
\end{cases}
\]

Funci\'on de Lagrange:
\[
\mathcal{L}(x,y,z,\lambda_1,\lambda_2)=x+2y+2z+
\lambda_1(25-x^2-y^2)+\lambda_2(-x-y-z)
\]

Puntos cr\'iticos:
\[
P_1=(5,0,-5),\quad
P_2=(-5,0,5)
\]

Clasificaci\'on:
\[
P_1:\text{m\'inimo relativo},\quad
P_2:\text{m\'aximo relativo}
\]
\end{Ejer}
%-------------------------------------------------
\begin{Ejer}

\[
\begin{cases}
f(x,y)=x^4+y^4\\
x-y=0
\end{cases}
\]

Funci\'on de Lagrange:
\[
\mathcal{L}(x,y,\lambda)=x^4+y^4+\lambda(-x+y)
\]

Punto cr\'itico:
\[
P=(0,0)
\]

Como $\det\bar H(P)=0$, no se puede concluir. Se requiere estudio local.
\end{Ejer}
